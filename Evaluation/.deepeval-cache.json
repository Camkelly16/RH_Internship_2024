{"test_cases_lookup_map": {"{\"actual_output\": \"A. rate(http_requests_total[1m])\", \"context\": null, \"expected_output\": \"rate(http_requests_total[1m])\", \"hyperparameters\": null, \"input\": \"What PromQL expression calculates the rate of HTTP requests over the last 1 minute?\", \"retrieval_context\": [\"\"]}": {"cached_metrics_data": [{"metric_metadata": {"metric": "Answer Relevancy", "threshold": 0.5, "success": true, "score": 0.5, "reason": "The score is 0.50 because the actual output includes an irrelevant statement about 'A.' which deviates from the original question asking for a PromQL expression to calculate the rate of HTTP requests over the last 1 minute.", "strictMode": false, "evaluationModel": "llama3", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "llama3", "strict_mode": false, "include_reason": true}}, {"metric_metadata": {"metric": "Faithfulness", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The score is 1.00 because there are no detected contradictions between the actual output and the retrieval context.", "strictMode": false, "evaluationModel": "llama3", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "llama3", "strict_mode": false, "include_reason": true}}]}}}